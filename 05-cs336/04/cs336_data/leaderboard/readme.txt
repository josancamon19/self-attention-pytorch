each warc.gz file is about 1GB of data, each one has about 25000 records, which after cleaning, reduce to like 10%
need 7B tokens, given @train.py, let's focus on that part given a few files, and with default cleaning stuff


Exploring Paloma dataset
separator is <|endoftext|>