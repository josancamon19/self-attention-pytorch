{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Mathematics Foundation\n",
    "\n",
    "**Basic Objects**\n",
    "- Scalar: Single number (5, -2.3)\n",
    "- Vector: Ordered list of numbers [1, 3, -2]\n",
    "- Matrix: Rectangular array of numbers with rows and columns\n",
    "\n",
    "**Strict Mathematical Rules**\n",
    "- Addition: Only between same-size objects\n",
    "  - Matrix + Matrix: Must have identical dimensions (2×3 + 2×3 ✓, 2×3 + 3×2 ✗)\n",
    "  - Vector + Vector: Must have same length ([1,2] + [3,4] ✓)\n",
    "  - Matrix + Vector: UNDEFINED in pure math\n",
    "\n",
    "- Multiplication:\n",
    "  - Scalar × Anything: Multiply every element\n",
    "  - Vector × Vector (dot product): Same length required, produces scalar\n",
    "    - `ab=ba` commutative\n",
    "  - Matrix × Matrix: Inner dimensions must match (m×n) × (n×p) → (m×p)\n",
    "    - `AxB` means A_row[i] dot product B_col[j]\n",
    "    - `AxB != BxA`\n",
    "    - `AxB` in pure math is only defined for 2D matrices.\n",
    "  - Matrix × Vector: Columns in matrix = elements in vector (2×3) × (3×1) → (2×1)\n",
    "\n",
    "- Division:\n",
    "  - Vector ÷ Vector: NOT DEFINED in pure mathematics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental Meaning\n",
    "- Vector as an arrow in space that tells you both which way to go and how far to travel.\n",
    "- direction and magnitude. Anything that can be represented as `how much` and `what direction`.\n",
    "- [3, 4], means go 3 steps in `x` direction, 4 steps in `y` direction\n",
    "- Matrix is a way to write transformation rules for vectors\n",
    "- `A x b` means we are using A matrix transformation rules to move b vector.\n",
    "  - How are we moving `b`?\n",
    "    - it depends on what `A` represents\n",
    "    - if (n, n) x (n,) = (n,) - vector dimensions are preserved, we can say either\n",
    "      - We kept the vector in it's coordinate system, and just moved it around (rotated, scaled, reflected)\n",
    "      - Or depending of what A represents, we could've kept the dimensions count, but moved the vector to a complete new coordinate system. (`projected`!)\n",
    "    - if (m, n) x (n, ) = (m, ) - we are definitely moving the vector to a new coordinate system\n",
    "    - **Example**\n",
    "      > Imagine you have data about students with three features: hours studied, hours slept, and number of practice problems completed. This is a vector in 3-dimensional space, like [8, 7, 15]. Now suppose you want to predict two outcomes: their test score and their stress level. You might use a transformation matrix that takes your 3-dimensional input and produces a 2-dimensional output.\n",
    "- But wait, what about a matrix as a collection of vectors instead?\n",
    "  - Yes, also true, it is a collection of numbers you can decide what to put in it.\n",
    "  - if the rows are being treated individually, then is a collection of vectors\n",
    "- `Q @ K^T`, is a collection of vectors, each vector at Q is computing similarities with K\n",
    "- But `W_Q @ X` is a transformation, where W_T is moving X to a new dimension space to provide Q.\n",
    "  - Fun fact: `W_Q @ X` is the correct mathematical way, but ML adopted `X @ W_Q` to represent it.\n",
    "- So what if A,B are matrices that don't represent list of vectors like in Attention, but are Transformations?\n",
    "  - `A @ B = C`, where C means, apply Transformation B, then apply A.\n",
    "  - This is what basic neural networks (FFN/MLP) do all the time.\n",
    "    - input → `W1` → intermediate → W2 → output\n",
    "    - Each of W matrices, learns to project the input into a space where x pattern becomes more visible.\n",
    "    - `W2 @ W1`, `W1` learns to create representations that are useful for `W2`, and `W2` learns how to use those representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Changes Algebra Math a little\n",
    "\n",
    "- Pure math matrix multiplication is represented as `@`\n",
    "  - Same as pure math, Linear transformation representations\n",
    "- Element wise multiplication is `*` (this doesn't exist in actual math)\n",
    "  - Used for scaling the values or mask them.\n",
    "- Element wise `+` and `-` remain the same, except that we have broadcasting.\n",
    "  - In ML, we introduce `/` as an operation (doesn't exist in pure math).\n",
    "  - Broadcasting is a pratical fix that ML does in libraries, in which if dimensions don't match, e.g. pytorch has some defined rules on how to match them.\n",
    "- There are no 3D, 4D matrices in math, we call them Tensors instead.\n",
    "- Matmul `@` in tensors still has the (mxn) x (nxp) = (m,p) rule, so internal dimensions must match,\n",
    "  - what does this mean in 3D, 4D?\n",
    "  - Last 2 dimensions do the magic here.\n",
    "  - (..., m, n) x (..., n, p) = `(..., m, p)`\n",
    "  - what about (...)? at each dim i, you take max(Ai_size,Bi_size)\n",
    "  - Pytorch can do broadcasting here as well as long as rule of last 2 dimensions is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Prove all of this in code with pytorch and numpy\n",
    "- [ ] Scaled dot attention and Variance\n",
    "- [ ] Include similarity notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
